{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from torch_geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from requests->torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from requests->torch_geometric) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\user\\anaconda3\\envs\\test\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib pandas numpy scikit-learn torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "# 그래프 데이터를 로드하는 함수\n",
    "def get_graph_data(edges_file, nodes_file):\n",
    "    \"\"\"\n",
    "    edges.txt와 nodes.csv 파일에서 그래프 데이터를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        edges_file (str): 엣지 데이터 파일 경로.\n",
    "        nodes_file (str): 노드 데이터 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        x (Tensor): 노드 특징 텐서.\n",
    "        edge_index (Tensor): 엣지 인덱스 텐서.\n",
    "    \"\"\"\n",
    "    # 노드와 엣지 데이터 로드\n",
    "    nodes = pd.read_csv(nodes_file)\n",
    "    edges = pd.read_csv(edges_file, sep=r'\\s+', header=None)\n",
    "\n",
    "    # 'id' 컬럼이 존재하는지 확인\n",
    "    if 'id' not in nodes.columns:\n",
    "        raise ValueError(\"nodes.csv 파일에 'id' 컬럼이 없습니다.\")\n",
    "\n",
    "    # 노드 ID를 인덱스로 매핑\n",
    "    node_id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes['id'])}\n",
    "    edges[0] = edges[0].map(node_id_to_idx)\n",
    "    edges[1] = edges[1].map(node_id_to_idx)\n",
    "\n",
    "    # 매핑 중 잘못된 데이터가 없는지 확인\n",
    "    if edges.isnull().values.any():\n",
    "        raise ValueError(\"엣지 데이터에 잘못된 노드 ID가 포함되어 있습니다.\")\n",
    "\n",
    "    # 노드 데이터를 PyTorch 텐서로 변환\n",
    "    x = torch.tensor(nodes['id'].values, dtype=torch.float).unsqueeze(1)  # 차원 추가\n",
    "\n",
    "    # 엣지 데이터를 PyTorch 텐서로 변환\n",
    "    edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "\n",
    "    return x, edge_index\n",
    "\n",
    "\n",
    "# 그래프 데이터셋 클래스 정의\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    그래프 데이터를 PyTorch Geometric의 Dataset 형식으로 정의합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        데이터셋 초기화.\n",
    "\n",
    "        Args:\n",
    "            root (str): 데이터셋 루트 디렉토리 경로.\n",
    "            transform (callable, optional): 데이터 변환 함수.\n",
    "            pre_transform (callable, optional): 데이터 전처리 함수.\n",
    "        \"\"\"\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.graph_files = []  # 그래프 파일 경로 저장 리스트\n",
    "        self.labels = []  # 그래프 라벨 저장 리스트\n",
    "\n",
    "        # 루트 디렉토리의 각 폴더를 탐색하여 그래프와 라벨 수집\n",
    "        for label_dir in os.listdir(root):\n",
    "            if \"Non_Conspiracy\" in label_dir:\n",
    "                label = 0  # Non-Conspiracy 폴더의 라벨\n",
    "            elif \"Conspiracy\" in label_dir:\n",
    "                label = 1  # Conspiracy 폴더의 라벨\n",
    "            else:\n",
    "                raise ValueError(f\"알 수 없는 라벨 디렉토리: {label_dir}\")\n",
    "\n",
    "            # 각 서브 디렉토리를 탐색하여 그래프 데이터 수집\n",
    "            subdir = os.path.join(root, label_dir)\n",
    "            for graph_index in os.listdir(subdir):\n",
    "                graph_path = os.path.join(subdir, graph_index)\n",
    "                if os.path.isdir(graph_path):  # 디렉토리인지 확인\n",
    "                    self.graph_files.append(graph_path)  # 그래프 경로 저장\n",
    "                    self.labels.append(label)  # 라벨 저장\n",
    "\n",
    "        # 라벨을 PyTorch 텐서로 변환\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 크기를 반환합니다.\n",
    "        \"\"\"\n",
    "        return len(self.graph_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"\n",
    "        데이터셋의 특정 인덱스에 해당하는 그래프 데이터를 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            idx (int): 그래프 인덱스.\n",
    "\n",
    "        Returns:\n",
    "            Data: PyTorch Geometric의 Data 객체.\n",
    "        \"\"\"\n",
    "        graph_path = self.graph_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 엣지 및 노드 파일 경로 설정\n",
    "        edges_file = os.path.join(graph_path, \"edges.txt\")\n",
    "        nodes_file = os.path.join(graph_path, \"nodes.csv\")\n",
    "\n",
    "        # 엣지와 노드 파일이 비어 있는 경우 처리\n",
    "        if not os.path.exists(edges_file) or os.stat(edges_file).st_size == 0:\n",
    "            edges = pd.DataFrame(columns=[0, 1])  # 빈 엣지 데이터 생성\n",
    "        else:\n",
    "            edges = pd.read_csv(edges_file, sep=r'\\s+', header=None)\n",
    "\n",
    "        if not os.path.exists(nodes_file) or os.stat(nodes_file).st_size == 0:\n",
    "            nodes = pd.DataFrame({'id': [0]})  # 기본 노드 생성\n",
    "        else:\n",
    "            nodes = pd.read_csv(nodes_file)\n",
    "\n",
    "        # 'id' 컬럼이 없는 경우 기본 값 추가\n",
    "        if 'id' not in nodes.columns:\n",
    "            nodes['id'] = range(len(nodes))\n",
    "\n",
    "        # 노드 ID를 인덱스로 매핑\n",
    "        node_id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes['id'])}\n",
    "        edges[0] = edges[0].map(node_id_to_idx)\n",
    "        edges[1] = edges[1].map(node_id_to_idx)\n",
    "\n",
    "        # 잘못된 엣지 데이터 제거\n",
    "        edges = edges.dropna().astype(int)\n",
    "\n",
    "        # PyTorch 텐서로 변환\n",
    "        x = torch.tensor(nodes['id'].values, dtype=torch.float).unsqueeze(1)\n",
    "        edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "\n",
    "        # PyTorch Geometric의 Data 객체 반환\n",
    "        return Data(x=x, edge_index=edge_index, y=label)\n",
    "\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "data_root = \"dataset\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = GraphDataset(root=data_root)\n",
    "\n",
    "# 데이터셋 크기와 라벨 출력\n",
    "print(f\"Total graphs: {len(dataset)}\")  # 전체 그래프 수\n",
    "print(f\"Labels: {dataset.labels.unique().tolist()}\")  # 라벨 분포\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋을 클래스 비율에 맞게 분할\n",
    "def split_dataset_by_class(dataset, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    # 클래스별 인덱스 수집\n",
    "    class_indices = {label: [] for label in np.unique([data.y.item() for data in dataset])}\n",
    "    for idx, data in enumerate(dataset):\n",
    "        class_indices[data.y.item()].append(idx)\n",
    "\n",
    "    # 각 클래스별로 학습, 검증, 테스트 인덱스 분할\n",
    "    train_indices, val_indices, test_indices = [], [], []\n",
    "    for label, indices in class_indices.items():\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "        train_idx, val_idx = train_test_split(train_idx, test_size=val_size / (1 - test_size), random_state=random_state)\n",
    "        train_indices.extend(train_idx)\n",
    "        val_indices.extend(val_idx)\n",
    "        test_indices.extend(test_idx)\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "# 시드 고정 (재현 가능한 결과를 위해 사용)\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_idx, val_idx, test_idx = split_dataset_by_class(dataset, test_size=0.2, val_size=0.1, random_state=random_state)\n",
    "\n",
    "# 분할된 인덱스로 데이터셋 생성\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # 학습용 데이터 섞음\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)                   # 검증 데이터 고정\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)                 # 테스트 데이터 고정\n",
    "\n",
    "# 데이터셋 크기 출력\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 시드 고정 함수\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 고정\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Seed set to: {SEED}\")\n",
    "\n",
    "# CUDA 확인 및 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# GCN 분류기 정의\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, \n",
    "                 num_layers=2, dropout_rate=0.5, use_batch_norm=True, use_residual=False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_residual = use_residual\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # GCN 레이어 정의\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        # 배치 정규화 레이어 정의\n",
    "        if use_batch_norm:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
    "\n",
    "        # 드롭아웃 정의\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 완전 연결 레이어\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        residual = None\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index).relu()\n",
    "            if self.use_batch_norm:\n",
    "                x = self.bns[i](x)\n",
    "            if self.use_residual and residual is not None:\n",
    "                x += residual\n",
    "            x = self.dropout(x)\n",
    "            residual = x\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 모델, 옵티마이저 및 손실 함수 초기화\n",
    "\n",
    "\n",
    "# 클래스 분포 확인 및 클래스 가중치 설정\n",
    "labels = [data.y.item() for data in dataset]\n",
    "class_counts = torch.tensor([labels.count(0), labels.count(1), labels.count(2)], dtype=torch.float)  # 클래스 3개\n",
    "class_weights = 1.0 / class_counts  # 클래스 가중치 계산\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))  # 가중치 적용된 손실 함수\n",
    "\n",
    "# GCN 분류기 초기화\n",
    "input_dim = dataset[0].x.shape[1]  # 노드 특징 차원\n",
    "model = GCNClassifier(input_dim=input_dim, hidden_dim=128, output_dim=2).to(device)  # 출력 차원 3으로 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "# 얼리 스토핑 설정\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training Loop: 학습 루프\n",
    "def train():\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0  # 총 손실 초기화\n",
    "    all_preds = []  # 전체 예측값 저장 리스트\n",
    "    all_labels = []  # 전체 실제 라벨 저장 리스트\n",
    "\n",
    "    for data in train_loader:  # 학습 데이터 로더에서 배치 단위로 데이터 가져오기\n",
    "        data = data.to(device)  # 데이터를 GPU 또는 CPU로 이동\n",
    "        optimizer.zero_grad()  # 이전 배치의 그래디언트 초기화\n",
    "        out = model(data)  # 모델에 데이터를 전달하여 출력값 계산\n",
    "        loss = criterion(out, data.y)  # 출력값과 실제 라벨을 비교하여 손실 계산\n",
    "        loss.backward()  # 손실에 대한 그래디언트 계산\n",
    "        optimizer.step()  # 옵티마이저를 사용하여 모델 파라미터 업데이트\n",
    "        total_loss += loss.item()  # 배치 손실 값을 총 손실에 더함\n",
    "\n",
    "        # 예측값과 실제 라벨 저장\n",
    "        pred = out.argmax(dim=1)  # 예측값(가장 높은 확률의 클래스 선택)\n",
    "        all_preds.extend(pred.cpu().numpy())  # 예측값을 CPU로 이동 후 저장\n",
    "        all_labels.extend(data.y.cpu().numpy())  # 실제 라벨을 CPU로 이동 후 저장\n",
    "    \n",
    "    # 정확도와 F1-score 계산\n",
    "    accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).sum().item() / len(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # 가중치가 있는 F1-score 계산\n",
    "    return total_loss / len(train_loader), accuracy, f1  # 평균 손실, 정확도, F1-score 반환\n",
    "# Validation/Test Loop: 검증 및 테스트 루프\n",
    "def validate(loader):\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)  # 데이터를 GPU/CPU로 이동\n",
    "            out = model(data)  # 모델 출력 계산\n",
    "            loss = criterion(out, data.y)  # 손실 계산\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 예측값과 확률 저장\n",
    "            pred = out.argmax(dim=1)\n",
    "            prob = out.softmax(dim=1)  # 모든 클래스의 확률 계산\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "            all_probs.extend(prob.cpu().numpy())\n",
    "\n",
    "    # 정확도와 F1-score 계산\n",
    "    accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).sum().item() / len(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # F1-score 계산\n",
    "\n",
    "    # AUC 계산 (다중 클래스)\n",
    "    auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\")  # One-vs-Rest 방식으로 AUC 계산\n",
    "    return total_loss / len(loader), accuracy, f1, auc\n",
    "\n",
    "# Main Training Script 유지\n",
    "for epoch in range(50):  # 최대 50 에포크 동안 학습\n",
    "    train_loss, train_acc, train_f1 = train()  # 학습\n",
    "    val_loss, val_acc, val_f1, val_auc = validate(val_loader)  # 검증\n",
    "    scheduler.step()  # 학습률 감소\n",
    "\n",
    "    # 얼리 스토핑 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # 최적 모델 저장\n",
    "        print(f\"Epoch {epoch + 1:03d} | Saving best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # 에포크 결과 출력\n",
    "    print(f\"Epoch {epoch + 1:03d} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "# 테스트 데이터셋 평가\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))  # 최적 모델 로드\n",
    "test_loss, test_acc, test_f1, test_auc = validate(test_loader)  # 테스트 데이터셋 검증\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}, Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

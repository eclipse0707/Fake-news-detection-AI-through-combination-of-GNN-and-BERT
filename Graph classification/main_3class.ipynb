{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total graphs: 3511\n",
      "Labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "# 그래프 데이터를 로드하는 함수\n",
    "def get_graph_data(edges_file, nodes_file):\n",
    "    \"\"\"\n",
    "    edges.txt와 nodes.csv 파일에서 그래프 데이터를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        edges_file (str): 엣지 데이터 파일 경로.\n",
    "        nodes_file (str): 노드 데이터 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        x (Tensor): 노드 특징 텐서.\n",
    "        edge_index (Tensor): 엣지 인덱스 텐서.\n",
    "    \"\"\"\n",
    "    # 노드와 엣지 데이터 로드\n",
    "    nodes = pd.read_csv(nodes_file)\n",
    "    edges = pd.read_csv(edges_file, sep=r'\\s+', header=None)\n",
    "\n",
    "    # 'id' 컬럼이 존재하는지 확인\n",
    "    if 'id' not in nodes.columns:\n",
    "        raise ValueError(\"nodes.csv 파일에 'id' 컬럼이 없습니다.\")\n",
    "\n",
    "    # 노드 ID를 인덱스로 매핑\n",
    "    node_id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes['id'])}\n",
    "    edges[0] = edges[0].map(node_id_to_idx)\n",
    "    edges[1] = edges[1].map(node_id_to_idx)\n",
    "\n",
    "    # 매핑 중 잘못된 데이터가 없는지 확인\n",
    "    if edges.isnull().values.any():\n",
    "        raise ValueError(\"엣지 데이터에 잘못된 노드 ID가 포함되어 있습니다.\")\n",
    "\n",
    "    # 노드 데이터를 PyTorch 텐서로 변환\n",
    "    x = torch.tensor(nodes['id'].values, dtype=torch.float).unsqueeze(1)  # 차원 추가\n",
    "\n",
    "    # 엣지 데이터를 PyTorch 텐서로 변환\n",
    "    edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "\n",
    "    return x, edge_index\n",
    "\n",
    "\n",
    "# 그래프 데이터셋 클래스 정의\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    그래프 데이터를 PyTorch Geometric의 Dataset 형식으로 정의합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        데이터셋 초기화.\n",
    "\n",
    "        Args:\n",
    "            root (str): 데이터셋 루트 디렉토리 경로.\n",
    "            transform (callable, optional): 데이터 변환 함수.\n",
    "            pre_transform (callable, optional): 데이터 전처리 함수.\n",
    "        \"\"\"\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.graph_files = []  # 그래프 파일 경로 저장 리스트\n",
    "        self.labels = []  # 그래프 라벨 저장 리스트\n",
    "\n",
    "        # 루트 디렉토리의 각 폴더를 탐색하여 그래프와 라벨 수집\n",
    "        for label_dir in os.listdir(root):\n",
    "            if \"Non_Conspiracy\" in label_dir:\n",
    "                label = 0  # Non-Conspiracy 폴더의 라벨\n",
    "            elif \"Conspiracy\" in label_dir:\n",
    "                label = 1  # Conspiracy 폴더의 라벨\n",
    "            elif \"Other\" in label_dir:\n",
    "                label = 2  # Other_Graphs 폴더의 라벨\n",
    "            else:\n",
    "                raise ValueError(f\"알 수 없는 라벨 디렉토리: {label_dir}\")\n",
    "\n",
    "            # 각 서브 디렉토리를 탐색하여 그래프 데이터 수집\n",
    "            subdir = os.path.join(root, label_dir)\n",
    "            for graph_index in os.listdir(subdir):\n",
    "                graph_path = os.path.join(subdir, graph_index)\n",
    "                if os.path.isdir(graph_path):  # 디렉토리인지 확인\n",
    "                    self.graph_files.append(graph_path)  # 그래프 경로 저장\n",
    "                    self.labels.append(label)  # 라벨 저장\n",
    "\n",
    "        # 라벨을 PyTorch 텐서로 변환\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 크기를 반환합니다.\n",
    "        \"\"\"\n",
    "        return len(self.graph_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"\n",
    "        데이터셋의 특정 인덱스에 해당하는 그래프 데이터를 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            idx (int): 그래프 인덱스.\n",
    "\n",
    "        Returns:\n",
    "            Data: PyTorch Geometric의 Data 객체.\n",
    "        \"\"\"\n",
    "        graph_path = self.graph_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 엣지 및 노드 파일 경로 설정\n",
    "        edges_file = os.path.join(graph_path, \"edges.txt\")\n",
    "        nodes_file = os.path.join(graph_path, \"nodes.csv\")\n",
    "\n",
    "        # 엣지와 노드 파일이 비어 있는 경우 처리\n",
    "        if not os.path.exists(edges_file) or os.stat(edges_file).st_size == 0:\n",
    "            edges = pd.DataFrame(columns=[0, 1])  # 빈 엣지 데이터 생성\n",
    "        else:\n",
    "            edges = pd.read_csv(edges_file, sep=r'\\s+', header=None)\n",
    "\n",
    "        if not os.path.exists(nodes_file) or os.stat(nodes_file).st_size == 0:\n",
    "            nodes = pd.DataFrame({'id': [0]})  # 기본 노드 생성\n",
    "        else:\n",
    "            nodes = pd.read_csv(nodes_file)\n",
    "\n",
    "        # 'id' 컬럼이 없는 경우 기본 값 추가\n",
    "        if 'id' not in nodes.columns:\n",
    "            nodes['id'] = range(len(nodes))\n",
    "\n",
    "        # 노드 ID를 인덱스로 매핑\n",
    "        node_id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes['id'])}\n",
    "        edges[0] = edges[0].map(node_id_to_idx)\n",
    "        edges[1] = edges[1].map(node_id_to_idx)\n",
    "\n",
    "        # 잘못된 엣지 데이터 제거\n",
    "        edges = edges.dropna().astype(int)\n",
    "\n",
    "        # PyTorch 텐서로 변환\n",
    "        x = torch.tensor(nodes['id'].values, dtype=torch.float).unsqueeze(1)\n",
    "        edge_index = torch.tensor(edges.values.T, dtype=torch.long)\n",
    "\n",
    "        # PyTorch Geometric의 Data 객체 반환\n",
    "        return Data(x=x, edge_index=edge_index, y=label)\n",
    "\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "data_root = \"dataset\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = GraphDataset(root=data_root)\n",
    "\n",
    "# 데이터셋 크기와 라벨 출력\n",
    "print(f\"Total graphs: {len(dataset)}\")  # 전체 그래프 수\n",
    "print(f\"Labels: {dataset.labels.unique().tolist()}\")  # 라벨 분포\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2454, Validation size: 353, Test size: 704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\test\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋을 클래스 비율에 맞게 분할\n",
    "def split_dataset_by_class(dataset, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    # 클래스별 인덱스 수집\n",
    "    class_indices = {label: [] for label in np.unique([data.y.item() for data in dataset])}\n",
    "    for idx, data in enumerate(dataset):\n",
    "        class_indices[data.y.item()].append(idx)\n",
    "\n",
    "    # 각 클래스별로 학습, 검증, 테스트 인덱스 분할\n",
    "    train_indices, val_indices, test_indices = [], [], []\n",
    "    for label, indices in class_indices.items():\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "        train_idx, val_idx = train_test_split(train_idx, test_size=val_size / (1 - test_size), random_state=random_state)\n",
    "        train_indices.extend(train_idx)\n",
    "        val_indices.extend(val_idx)\n",
    "        test_indices.extend(test_idx)\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "# 시드 고정 (재현 가능한 결과를 위해 사용)\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_idx, val_idx, test_idx = split_dataset_by_class(dataset, test_size=0.2, val_size=0.1, random_state=random_state)\n",
    "\n",
    "# 분할된 인덱스로 데이터셋 생성\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # 학습용 데이터 섞음\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)                   # 검증 데이터 고정\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)                 # 테스트 데이터 고정\n",
    "\n",
    "# 데이터셋 크기 출력\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to: 42\n",
      "Using device: cuda\n",
      "Epoch 001 | Saving best model\n",
      "Epoch 001 | Train Loss: 1.0954, Train Acc: 0.5338, Train F1: 0.5734 | Val Loss: 1.1098, Val Acc: 0.3994, Val F1: 0.4509, Val AUC: 0.6075\n",
      "Epoch 002 | Train Loss: 1.0969, Train Acc: 0.5024, Train F1: 0.5482 | Val Loss: 1.1603, Val Acc: 0.4193, Val F1: 0.4586, Val AUC: 0.5908\n",
      "Epoch 003 | Saving best model\n",
      "Epoch 003 | Train Loss: 1.0794, Train Acc: 0.5130, Train F1: 0.5546 | Val Loss: 1.0343, Val Acc: 0.5042, Val F1: 0.5190, Val AUC: 0.6344\n",
      "Epoch 004 | Saving best model\n",
      "Epoch 004 | Train Loss: 1.0770, Train Acc: 0.5473, Train F1: 0.5818 | Val Loss: 0.9876, Val Acc: 0.6091, Val F1: 0.5953, Val AUC: 0.5934\n",
      "Epoch 005 | Train Loss: 1.0661, Train Acc: 0.5656, Train F1: 0.5966 | Val Loss: 1.1166, Val Acc: 0.3853, Val F1: 0.4037, Val AUC: 0.6095\n",
      "Epoch 006 | Train Loss: 1.0628, Train Acc: 0.5363, Train F1: 0.5739 | Val Loss: 1.0537, Val Acc: 0.4844, Val F1: 0.5219, Val AUC: 0.6216\n",
      "Epoch 007 | Train Loss: 1.0673, Train Acc: 0.5509, Train F1: 0.5882 | Val Loss: 1.0468, Val Acc: 0.4646, Val F1: 0.5215, Val AUC: 0.6258\n",
      "Epoch 008 | Train Loss: 1.0646, Train Acc: 0.5518, Train F1: 0.5861 | Val Loss: 1.0168, Val Acc: 0.5354, Val F1: 0.5418, Val AUC: 0.6339\n",
      "Epoch 009 | Train Loss: 1.0589, Train Acc: 0.5795, Train F1: 0.6034 | Val Loss: 1.1002, Val Acc: 0.3824, Val F1: 0.4069, Val AUC: 0.6405\n",
      "Epoch 010 | Train Loss: 1.0615, Train Acc: 0.5570, Train F1: 0.5933 | Val Loss: 1.0401, Val Acc: 0.5411, Val F1: 0.5484, Val AUC: 0.6228\n",
      "Epoch 011 | Saving best model\n",
      "Epoch 011 | Train Loss: 1.0569, Train Acc: 0.5509, Train F1: 0.5840 | Val Loss: 0.9855, Val Acc: 0.6034, Val F1: 0.6004, Val AUC: 0.6411\n",
      "Epoch 012 | Train Loss: 1.0671, Train Acc: 0.5676, Train F1: 0.5920 | Val Loss: 1.1586, Val Acc: 0.3258, Val F1: 0.3291, Val AUC: 0.6360\n",
      "Epoch 013 | Train Loss: 1.0624, Train Acc: 0.5607, Train F1: 0.5938 | Val Loss: 1.0236, Val Acc: 0.5722, Val F1: 0.5976, Val AUC: 0.6326\n",
      "Epoch 014 | Train Loss: 1.0609, Train Acc: 0.5587, Train F1: 0.5876 | Val Loss: 1.0716, Val Acc: 0.4278, Val F1: 0.4652, Val AUC: 0.6290\n",
      "Epoch 015 | Train Loss: 1.0533, Train Acc: 0.5676, Train F1: 0.5985 | Val Loss: 0.9903, Val Acc: 0.6006, Val F1: 0.5941, Val AUC: 0.6363\n",
      "Epoch 016 | Train Loss: 1.0575, Train Acc: 0.5652, Train F1: 0.5970 | Val Loss: 1.0087, Val Acc: 0.5807, Val F1: 0.5787, Val AUC: 0.6332\n",
      "Epoch 017 | Train Loss: 1.0612, Train Acc: 0.5660, Train F1: 0.5958 | Val Loss: 0.9927, Val Acc: 0.5694, Val F1: 0.5853, Val AUC: 0.6378\n",
      "Epoch 018 | Train Loss: 1.0600, Train Acc: 0.5795, Train F1: 0.5999 | Val Loss: 1.0730, Val Acc: 0.4391, Val F1: 0.4596, Val AUC: 0.6431\n",
      "Epoch 019 | Train Loss: 1.0616, Train Acc: 0.5395, Train F1: 0.5761 | Val Loss: 0.9989, Val Acc: 0.5694, Val F1: 0.5705, Val AUC: 0.6408\n",
      "Epoch 020 | Train Loss: 1.0592, Train Acc: 0.5583, Train F1: 0.5859 | Val Loss: 1.0584, Val Acc: 0.5071, Val F1: 0.5323, Val AUC: 0.6356\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4248\\156069754.py:171: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))  # 최적 모델 로드\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9492, Test Acc: 0.6080, Test F1: 0.6110, Test AUC: 0.6838\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 시드 고정 함수\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 고정\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Seed set to: {SEED}\")\n",
    "\n",
    "# CUDA 확인 및 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# GCN 분류기 정의\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, \n",
    "                 num_layers=2, dropout_rate=0.5, use_batch_norm=True, use_residual=False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_residual = use_residual\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # GCN 레이어 정의\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        # 배치 정규화 레이어 정의\n",
    "        if use_batch_norm:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
    "\n",
    "        # 드롭아웃 정의\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 완전 연결 레이어\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        residual = None\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index).relu()\n",
    "            if self.use_batch_norm:\n",
    "                x = self.bns[i](x)\n",
    "            if self.use_residual and residual is not None:\n",
    "                x += residual\n",
    "            x = self.dropout(x)\n",
    "            residual = x\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 모델, 옵티마이저 및 손실 함수 초기화\n",
    "\n",
    "\n",
    "# 클래스 분포 확인 및 클래스 가중치 설정\n",
    "labels = [data.y.item() for data in dataset]\n",
    "class_counts = torch.tensor([labels.count(0), labels.count(1), labels.count(2)], dtype=torch.float)  # 클래스 3개\n",
    "class_weights = 1.0 / class_counts  # 클래스 가중치 계산\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))  # 가중치 적용된 손실 함수\n",
    "\n",
    "# GCN 분류기 초기화\n",
    "input_dim = dataset[0].x.shape[1]  # 노드 특징 차원\n",
    "model = GCNClassifier(input_dim=input_dim, hidden_dim=128, output_dim=3).to(device)  # 출력 차원 3으로 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "# 얼리 스토핑 설정\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training Loop: 학습 루프\n",
    "def train():\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0  # 총 손실 초기화\n",
    "    all_preds = []  # 전체 예측값 저장 리스트\n",
    "    all_labels = []  # 전체 실제 라벨 저장 리스트\n",
    "\n",
    "    for data in train_loader:  # 학습 데이터 로더에서 배치 단위로 데이터 가져오기\n",
    "        data = data.to(device)  # 데이터를 GPU 또는 CPU로 이동\n",
    "        optimizer.zero_grad()  # 이전 배치의 그래디언트 초기화\n",
    "        out = model(data)  # 모델에 데이터를 전달하여 출력값 계산\n",
    "        loss = criterion(out, data.y)  # 출력값과 실제 라벨을 비교하여 손실 계산\n",
    "        loss.backward()  # 손실에 대한 그래디언트 계산\n",
    "        optimizer.step()  # 옵티마이저를 사용하여 모델 파라미터 업데이트\n",
    "        total_loss += loss.item()  # 배치 손실 값을 총 손실에 더함\n",
    "\n",
    "        # 예측값과 실제 라벨 저장\n",
    "        pred = out.argmax(dim=1)  # 예측값(가장 높은 확률의 클래스 선택)\n",
    "        all_preds.extend(pred.cpu().numpy())  # 예측값을 CPU로 이동 후 저장\n",
    "        all_labels.extend(data.y.cpu().numpy())  # 실제 라벨을 CPU로 이동 후 저장\n",
    "    \n",
    "    # 정확도와 F1-score 계산\n",
    "    accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).sum().item() / len(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # 가중치가 있는 F1-score 계산\n",
    "    return total_loss / len(train_loader), accuracy, f1  # 평균 손실, 정확도, F1-score 반환\n",
    "# Validation/Test Loop: 검증 및 테스트 루프\n",
    "def validate(loader):\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)  # 데이터를 GPU/CPU로 이동\n",
    "            out = model(data)  # 모델 출력 계산\n",
    "            loss = criterion(out, data.y)  # 손실 계산\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 예측값과 확률 저장\n",
    "            pred = out.argmax(dim=1)\n",
    "            prob = out.softmax(dim=1)  # 모든 클래스의 확률 계산\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "            all_probs.extend(prob.cpu().numpy())\n",
    "\n",
    "    # 정확도와 F1-score 계산\n",
    "    accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).sum().item() / len(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # F1-score 계산\n",
    "\n",
    "    # AUC 계산 (다중 클래스)\n",
    "    auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\")  # One-vs-Rest 방식으로 AUC 계산\n",
    "    return total_loss / len(loader), accuracy, f1, auc\n",
    "\n",
    "# Main Training Script 유지\n",
    "for epoch in range(50):  # 최대 50 에포크 동안 학습\n",
    "    train_loss, train_acc, train_f1 = train()  # 학습\n",
    "    val_loss, val_acc, val_f1, val_auc = validate(val_loader)  # 검증\n",
    "    scheduler.step()  # 학습률 감소\n",
    "\n",
    "    # 얼리 스토핑 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # 최적 모델 저장\n",
    "        print(f\"Epoch {epoch + 1:03d} | Saving best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # 에포크 결과 출력\n",
    "    print(f\"Epoch {epoch + 1:03d} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "# 테스트 데이터셋 평가\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))  # 최적 모델 로드\n",
    "test_loss, test_acc, test_f1, test_auc = validate(test_loader)  # 테스트 데이터셋 검증\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}, Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3824025602.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Test Loss: 0.5564, Test Acc: 0.7397, Test F1: 0.7665, Test AUC: 0.7179\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Test Loss: 0.5564, Test Acc: 0.7397, Test F1: 0.7665, Test AUC: 0.7179"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
